{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a311d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoProcessor\n",
    "\n",
    "\n",
    "# ===================== 配置区 =====================\n",
    "model_path = \"/workspace/models/Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "file_path = \"/workspace/probing/Probing/data/test_case.jsonl\"\n",
    "base_image_dir = \"/workspace/\"\n",
    "\n",
    "QUESTION = \"这幅画的名称和作者是谁？\" \n",
    "\n",
    "batch_size = 4\n",
    "epochs = 10              # 建议多训几轮看看趋势\n",
    "lr = 1e-3\n",
    "max_pixels = 80_000_000  # 控制单图像像素上限，防炸\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 加载模型 & processor =====================\n",
    "print(\"Loading tokenizer & model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=None,\n",
    ").to(device).eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "hidden_size = model.config.hidden_size\n",
    "print(\"Hidden size:\", hidden_size)\n",
    "print(\"Model param device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 构造样本 =====================\n",
    "samples = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        item = json.loads(line)\n",
    "\n",
    "        filename = item[\"filename\"]\n",
    "        work_title = item[\"work_title\"].strip()\n",
    "        author_name_cn = item[\"author_name_cn\"].strip()\n",
    "\n",
    "        # label 字符串：作品名 + 作者，作为一个类别\n",
    "        label_str = f\"<作品名>:{work_title}<作者名>:{author_name_cn}\"\n",
    "\n",
    "        image_path = os.path.join(base_image_dir, filename)\n",
    "\n",
    "        samples.append({\n",
    "            \"image_path\": image_path,         # 图片路径\n",
    "            \"question\": QUESTION,             # 纯问题，不含 <image> / {t}\n",
    "            \"work_title\": work_title,\n",
    "            \"author_name_cn\": author_name_cn,\n",
    "            \"label\": label_str,               # 用来做 label2id\n",
    "        })\n",
    "\n",
    "print(\"Total samples:\", len(samples))\n",
    "if len(samples) == 0:\n",
    "    raise RuntimeError(\"No samples loaded, please check file_path / jsonl format.\")\n",
    "\n",
    "print(\"Example sample:\", samples[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5acb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 划分训练 / 验证 & label 映射 =====================\n",
    "random.seed(42)\n",
    "random.shuffle(samples)\n",
    "\n",
    "split = int(0.8 * len(samples))\n",
    "train_samples = samples[:split]\n",
    "val_samples = samples[split:]\n",
    "\n",
    "labels = sorted(list({s[\"label\"] for s in samples}))\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "print(\"Num classes:\", len(label2id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Dataset & Collate =====================\n",
    "class QwenVLProbeDataset(Dataset):\n",
    "    def __init__(self, samples, label2id, max_pixels=80_000_000):\n",
    "        self.samples = samples\n",
    "        self.label2id = label2id\n",
    "        self.max_pixels = max_pixels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.samples[idx]\n",
    "\n",
    "        # 1. 读图 & 防 DecompressionBomb\n",
    "        image = Image.open(s[\"image_path\"]).convert(\"RGB\")\n",
    "        if image.width * image.height > self.max_pixels:\n",
    "            scale = (self.max_pixels / (image.width * image.height)) ** 0.5\n",
    "            new_w = int(image.width * scale)\n",
    "            new_h = int(image.height * scale)\n",
    "            image = image.resize((new_w, new_h), Image.Resampling.BILINEAR)\n",
    "\n",
    "        # 2. 文本：纯问题\n",
    "        text = s[\"question\"]\n",
    "\n",
    "        # 3. 标签 id\n",
    "        label_id = self.label2id[s[\"label\"]]\n",
    "\n",
    "        return image, text, label_id\n",
    "\n",
    "\n",
    "def make_collate_fn(processor):\n",
    "    def collate_fn(batch):\n",
    "        images, texts, labels = zip(*batch)\n",
    "\n",
    "        # 官方的 image_token（如果没有则退回 \"<image>\"）\n",
    "        image_token = getattr(processor, \"image_token\", \"<image>\")\n",
    "\n",
    "        # 真正喂给模型的文本：只在这里加一次 image_token\n",
    "        # 例如 \"<image>\\n这幅画的名称和作者是谁？\"\n",
    "        texts_for_model = [f\"{image_token}\\n{t}\" for t in texts]\n",
    "\n",
    "        enc = processor(\n",
    "            images=list(images),\n",
    "            text=texts_for_model,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        return enc, labels\n",
    "\n",
    "    return collate_fn\n",
    "\n",
    "\n",
    "train_dataset = QwenVLProbeDataset(train_samples, label2id, max_pixels=max_pixels)\n",
    "val_dataset   = QwenVLProbeDataset(val_samples, label2id, max_pixels=max_pixels)\n",
    "\n",
    "collate_fn = make_collate_fn(processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838221bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 特征提取（多层 + mean pooling） =====================\n",
    "class QwenVLFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    从 Qwen2.5-VL Instruct 模型中抽特征：\n",
    "    1. 取最后 top_k_layers 层 hidden state 做平均\n",
    "    2. 在 token 维度上做带 attention_mask 的 mean pooling\n",
    "    这样可以更充分利用 cross-modal 融合后的表示。\n",
    "    \"\"\"\n",
    "    def __init__(self, qwen_model, top_k_layers=4):\n",
    "        super().__init__()\n",
    "        self.qwen = qwen_model\n",
    "        self.top_k_layers = top_k_layers\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: processor 的输出 dict（包含 input_ids, attention_mask, pixel_values 等）\n",
    "        device = next(self.qwen.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        outputs = self.qwen(\n",
    "            **inputs,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        # hidden_states: tuple of (num_layers+1, B, T, D)，第0个通常是 embedding\n",
    "        hidden_states = outputs.hidden_states   # len = num_layers + 1\n",
    "\n",
    "        # 取最后 top_k_layers 层（不含 embedding 层）\n",
    "        hs = hidden_states[1:]  # 去掉 embedding 层\n",
    "        num_hidden_layers = len(hs)\n",
    "\n",
    "        k = min(self.top_k_layers, num_hidden_layers)\n",
    "        last_k = hs[-k:]  # list of (B, T, D)\n",
    "\n",
    "        # 堆叠后在“层”维度做平均 -> (B, T, D)\n",
    "        stacked = torch.stack(last_k, dim=0)  # (k, B, T, D)\n",
    "        h = stacked.mean(dim=0)              # (B, T, D)\n",
    "\n",
    "        # 带 attention_mask 的 mean pooling：更稳定\n",
    "        attn = inputs.get(\"attention_mask\", torch.ones(h.size()[:2], device=h.device))  # (B, T)\n",
    "        attn = attn.unsqueeze(-1)  # (B, T, 1)\n",
    "\n",
    "        # 防止全 0\n",
    "        attn_sum = attn.sum(dim=1).clamp(min=1.0)  # (B, 1)\n",
    "\n",
    "        features = (h * attn).sum(dim=1) / attn_sum  # (B, D)\n",
    "        return features.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05224e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================== 线性 probe =====================\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(input_dim, num_labels)\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        # 可选：先 L2 normalize 一下，稳定一点\n",
    "        features = F.normalize(features, dim=-1)\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "feature_extractor = QwenVLFeatureExtractor(model, top_k_layers=4)\n",
    "probe = LinearProbe(hidden_size, num_labels=len(label2id)).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(probe.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================== 评估函数 =====================\n",
    "@torch.no_grad()\n",
    "def evaluate(probe, feature_extractor, data_loader, device):\n",
    "    probe.eval()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for enc, labels in data_loader:\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        features = feature_extractor(enc)\n",
    "        logits, loss = probe(features, labels)\n",
    "\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    avg_acc = total_correct / total_count if total_count > 0 else 0.0\n",
    "    avg_loss = total_loss / total_count if total_count > 0 else 0.0\n",
    "    return avg_acc, avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================== 训练主循环 =====================\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "for epoch in range(epochs):\n",
    "    probe.train()\n",
    "    total_loss = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    for batch_idx, (enc, labels) in enumerate(train_loader):\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 冻结大模型，只在 probe 上反向\n",
    "        with torch.no_grad():\n",
    "            features = feature_extractor(enc)\n",
    "\n",
    "        logits, loss = probe(features, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_count += bs\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "                f\"Batch {batch_idx+1}/{len(train_loader)} \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    train_loss = total_loss / total_count if total_count > 0 else 0.0\n",
    "    val_acc, val_loss = evaluate(probe, feature_extractor, val_loader, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Docker)",
   "language": "python",
   "name": "docker_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
